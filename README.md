# ğŸ¤– RAG Q&A Bot - Document Question Answering System

A production-ready **Retrieval-Augmented Generation (RAG)** pipeline built with Python, featuring semantic search, document chunking, and modular architecture.

## ğŸ¯ What is RAG?

RAG (Retrieval-Augmented Generation) combines:
- **Retrieval**: Finding relevant information from documents using semantic search
- **Generation**: Using LLMs to generate accurate answers based on retrieved context

This prevents hallucinations and grounds AI responses in your actual documents.

## âœ¨ Features

- ğŸ“„ **Document Chunking**: Multiple strategies (sentences, paragraphs, fixed-size)
- ğŸ§  **Semantic Search**: FAISS vector database with cosine similarity
- ğŸ” **Context Retrieval**: Find most relevant document chunks for queries
- ğŸ¯ **Modular Architecture**: Easy to extend and customize
- âš¡ **Production Ready**: Clean separation of concerns, typed interfaces

## ğŸ—ï¸ Architecture

```
Document â†’ Chunking â†’ Embeddings â†’ Vector Store
                                        â†“
Query â†’ Embed Query â†’ Semantic Search â†’ Top-K Chunks â†’ LLM â†’ Answer
```

### Project Structure

```
rag-qa-bot/
â”œâ”€â”€ models/              # Core components
â”‚   â”œâ”€â”€ embeddings.py    # Embedding models (mock & real)
â”‚   â”œâ”€â”€ chunker.py       # Document chunking strategies
â”‚   â”œâ”€â”€ vector_store.py  # FAISS vector database
â”‚   â””â”€â”€ llm.py          # LLM interface
â”œâ”€â”€ pipeline/            # RAG workflow
â”‚   â”œâ”€â”€ ingestion.py     # Document processing
â”‚   â”œâ”€â”€ retrieval.py     # Semantic search
â”‚   â””â”€â”€ generation.py    # Answer generation
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ config.py        # Configuration management
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ demo.py          # Demo scripts
â”œâ”€â”€ main.py              # RAG pipeline orchestration
â”œâ”€â”€ app.py               # Web API (optional)
â””â”€â”€ requirements.txt
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- pip

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/rag-qa-bot.git
cd rag-qa-bot

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Run Demo

```bash
python examples/demo.py
```

## ğŸ“– Usage

### Basic Usage

```python
from main import RAGPipeline
from utils.config import RAGConfig

# Initialize pipeline
config = RAGConfig(chunk_size=500, top_k=3)
rag = RAGPipeline(config)

# Add documents
rag.add_document(
    text="Your document content here...",
    metadata={'source': 'doc1.txt'}
)

# Ask questions
result = rag.query("What is machine learning?")
print(result['answer'])
print(f"Sources: {result['num_sources']}")
```

### Advanced Configuration

```python
from utils.config import RAGConfig

config = RAGConfig(
    chunk_size=500,           # Characters per chunk
    chunk_overlap=50,         # Overlap between chunks
    chunking_method='sentences',  # 'sentences', 'paragraphs', or 'fixed'
    top_k=3,                  # Number of chunks to retrieve
    embedding_dim=384,        # Embedding dimension
    max_tokens=500           # Max tokens for LLM response
)

rag = RAGPipeline(config)
```

### Different Chunking Strategies

```python
# Sentence-based chunking (default)
config = RAGConfig(chunking_method='sentences', chunk_size=500)

# Paragraph-based chunking
config = RAGConfig(chunking_method='paragraphs', chunk_size=800)

# Fixed-size chunking
config = RAGConfig(chunking_method='fixed', chunk_size=400)
```

## ğŸ”§ Upgrading to Production

### Use Real Embeddings (Sentence Transformers)

Update `models/embeddings.py`:

```python
from sentence_transformers import SentenceTransformer

class RealEmbedding:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.dim = self.model.get_sentence_embedding_dimension()
    
    def embed_text(self, text: str):
        return self.model.encode(text)
    
    def embed_batch(self, texts: list):
        return self.model.encode(texts)
```

Then in `main.py`:
```python
from models.embeddings import RealEmbedding
# ...
self.embedding_model = RealEmbedding()
```

### Use Real LLM (OpenAI)

```bash
pip install openai
```

Update `models/llm.py`:

```python
from openai import OpenAI

class OpenAILLM:
    def __init__(self, api_key=None):
        self.client = OpenAI(api_key=api_key)
    
    def generate(self, prompt: str, max_tokens: int = 500):
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
```

Set your API key:
```bash
# Windows PowerShell
$env:OPENAI_API_KEY="your-api-key"

# Linux/Mac
export OPENAI_API_KEY="your-api-key"
```

## ğŸ“Š Key Concepts Demonstrated

- âœ… **Document Chunking**: Breaking documents into manageable pieces
- âœ… **Embeddings**: Converting text to vector representations
- âœ… **Semantic Search**: Finding similar content using cosine similarity
- âœ… **Vector Database**: FAISS for efficient similarity search
- âœ… **Context Window Management**: Retrieving relevant chunks for LLM
- âœ… **Prompt Engineering**: Constructing effective prompts with context

## ğŸ§ª Testing

Run basic demo:
```bash
python examples/demo.py
```

Run advanced demo (tests multiple chunking strategies):
```python
# In examples/demo.py, uncomment:
# run_advanced_demo()
```

## ğŸ“ Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `chunk_size` | 500 | Characters per chunk |
| `chunk_overlap` | 50 | Overlap between chunks |
| `chunking_method` | 'sentences' | Chunking strategy |
| `top_k` | 3 | Number of chunks to retrieve |
| `embedding_dim` | 384 | Embedding vector dimension |
| `score_threshold` | 0.0 | Minimum similarity score |
| `max_tokens` | 500 | Max LLM response tokens |

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**
- **NumPy**: Vector operations
- **FAISS**: Vector similarity search
- **Sentence Transformers**: Text embeddings (optional)
- **OpenAI/Anthropic**: LLM APIs (optional)

## ğŸ“š Use Cases

- ğŸ“„ **Document Q&A**: Answer questions from company documents
- ğŸ“– **Knowledge Base**: Search through documentation
- ğŸ“ **Study Assistant**: Query textbooks and notes
- ğŸ’¼ **Legal/Medical**: Search through specialized documents
- ğŸ“° **News Analysis**: Query large article collections

## ğŸ”® Future Enhancements

- [ ] Add support for PDF/Word documents
- [ ] Implement query expansion
- [ ] Add re-ranking of results
- [ ] Cache embeddings for faster retrieval
- [ ] Add web interface (Streamlit/Gradio)
- [ ] Support for multiple document formats
- [ ] Hybrid search (keyword + semantic)
- [ ] Add conversation memory

## ğŸ¤ Contributing

Contributions welcome! Feel free to:
- Report bugs
- Suggest features
- Submit pull requests

## ğŸ“„ License

MIT License - feel free to use in your projects!

## ğŸ™ Acknowledgments

Built to demonstrate RAG fundamentals for production LLM applications.

Inspired by:
- LangChain
- LlamaIndex
- OpenAI RAG best practices

## ğŸ“ Contact

Questions or feedback? Open an issue or reach out!

---

â­ **Star this repo if you found it helpful!**